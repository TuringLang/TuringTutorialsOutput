<!DOCTYPE html>
<HTML lang = "en">
<HEAD>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Variational Inference</title>
  

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>

  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  
<style>
pre.hljl {
    border: 1px solid #ccc;
    margin: 5px;
    padding: 5px;
    overflow-x: auto;
    color: rgb(68,68,68); background-color: rgb(251,251,251); }
pre.hljl > span.hljl-t { }
pre.hljl > span.hljl-w { }
pre.hljl > span.hljl-e { }
pre.hljl > span.hljl-eB { }
pre.hljl > span.hljl-o { }
pre.hljl > span.hljl-k { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kc { color: rgb(59,151,46); font-style: italic; }
pre.hljl > span.hljl-kd { color: rgb(214,102,97); font-style: italic; }
pre.hljl > span.hljl-kn { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kp { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kr { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kt { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-n { }
pre.hljl > span.hljl-na { }
pre.hljl > span.hljl-nb { }
pre.hljl > span.hljl-nbp { }
pre.hljl > span.hljl-nc { }
pre.hljl > span.hljl-ncB { }
pre.hljl > span.hljl-nd { color: rgb(214,102,97); }
pre.hljl > span.hljl-ne { }
pre.hljl > span.hljl-neB { }
pre.hljl > span.hljl-nf { color: rgb(66,102,213); }
pre.hljl > span.hljl-nfm { color: rgb(66,102,213); }
pre.hljl > span.hljl-np { }
pre.hljl > span.hljl-nl { }
pre.hljl > span.hljl-nn { }
pre.hljl > span.hljl-no { }
pre.hljl > span.hljl-nt { }
pre.hljl > span.hljl-nv { }
pre.hljl > span.hljl-nvc { }
pre.hljl > span.hljl-nvg { }
pre.hljl > span.hljl-nvi { }
pre.hljl > span.hljl-nvm { }
pre.hljl > span.hljl-l { }
pre.hljl > span.hljl-ld { color: rgb(148,91,176); font-style: italic; }
pre.hljl > span.hljl-s { color: rgb(201,61,57); }
pre.hljl > span.hljl-sa { color: rgb(201,61,57); }
pre.hljl > span.hljl-sb { color: rgb(201,61,57); }
pre.hljl > span.hljl-sc { color: rgb(201,61,57); }
pre.hljl > span.hljl-sd { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdB { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdC { color: rgb(201,61,57); }
pre.hljl > span.hljl-se { color: rgb(59,151,46); }
pre.hljl > span.hljl-sh { color: rgb(201,61,57); }
pre.hljl > span.hljl-si { }
pre.hljl > span.hljl-so { color: rgb(201,61,57); }
pre.hljl > span.hljl-sr { color: rgb(201,61,57); }
pre.hljl > span.hljl-ss { color: rgb(201,61,57); }
pre.hljl > span.hljl-ssB { color: rgb(201,61,57); }
pre.hljl > span.hljl-nB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nbB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nfB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nh { color: rgb(59,151,46); }
pre.hljl > span.hljl-ni { color: rgb(59,151,46); }
pre.hljl > span.hljl-nil { color: rgb(59,151,46); }
pre.hljl > span.hljl-noB { color: rgb(59,151,46); }
pre.hljl > span.hljl-oB { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-ow { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-p { }
pre.hljl > span.hljl-c { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-ch { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cm { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cp { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cpB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cs { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-csB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-g { }
pre.hljl > span.hljl-gd { }
pre.hljl > span.hljl-ge { }
pre.hljl > span.hljl-geB { }
pre.hljl > span.hljl-gh { }
pre.hljl > span.hljl-gi { }
pre.hljl > span.hljl-go { }
pre.hljl > span.hljl-gp { }
pre.hljl > span.hljl-gs { }
pre.hljl > span.hljl-gsB { }
pre.hljl > span.hljl-gt { }
</style>



  <style type="text/css">
  @font-face {
  font-style: normal;
  font-weight: 300;
}
@font-face {
  font-style: normal;
  font-weight: 400;
}
@font-face {
  font-style: normal;
  font-weight: 600;
}
html {
  font-family: sans-serif; /* 1 */
  -ms-text-size-adjust: 100%; /* 2 */
  -webkit-text-size-adjust: 100%; /* 2 */
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block; /* 1 */
  vertical-align: baseline; /* 2 */
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 70%;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit; /* 1 */
  font: inherit; /* 2 */
  margin: 0; /* 3 */
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button; /* 2 */
  cursor: pointer; /* 3 */
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box; /* 1 */
  padding: 0; /* 2 */
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield; /* 1 */
  -moz-box-sizing: content-box;
  -webkit-box-sizing: content-box; /* 2 */
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0; /* 1 */
  padding: 0; /* 2 */
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  font-family: monospace, monospace;
  font-size : 0.8em;
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
thead th {
    border-bottom: 1px solid black;
    background-color: white;
}
tr:nth-child(odd){
  background-color: rgb(248,248,248);
}


/*
* Skeleton V2.0.4
* Copyright 2014, Dave Gamache
* www.getskeleton.com
* Free to use under the MIT license.
* http://www.opensource.org/licenses/mit-license.php
* 12/29/2014
*/
.container {
  position: relative;
  width: 100%;
  max-width: 960px;
  margin: 0 auto;
  padding: 0 20px;
  box-sizing: border-box; }
.column,
.columns {
  width: 100%;
  float: left;
  box-sizing: border-box; }
@media (min-width: 400px) {
  .container {
    width: 85%;
    padding: 0; }
}
@media (min-width: 550px) {
  .container {
    width: 80%; }
  .column,
  .columns {
    margin-left: 4%; }
  .column:first-child,
  .columns:first-child {
    margin-left: 0; }

  .one.column,
  .one.columns                    { width: 4.66666666667%; }
  .two.columns                    { width: 13.3333333333%; }
  .three.columns                  { width: 22%;            }
  .four.columns                   { width: 30.6666666667%; }
  .five.columns                   { width: 39.3333333333%; }
  .six.columns                    { width: 48%;            }
  .seven.columns                  { width: 56.6666666667%; }
  .eight.columns                  { width: 65.3333333333%; }
  .nine.columns                   { width: 74.0%;          }
  .ten.columns                    { width: 82.6666666667%; }
  .eleven.columns                 { width: 91.3333333333%; }
  .twelve.columns                 { width: 100%; margin-left: 0; }

  .one-third.column               { width: 30.6666666667%; }
  .two-thirds.column              { width: 65.3333333333%; }

  .one-half.column                { width: 48%; }

  /* Offsets */
  .offset-by-one.column,
  .offset-by-one.columns          { margin-left: 8.66666666667%; }
  .offset-by-two.column,
  .offset-by-two.columns          { margin-left: 17.3333333333%; }
  .offset-by-three.column,
  .offset-by-three.columns        { margin-left: 26%;            }
  .offset-by-four.column,
  .offset-by-four.columns         { margin-left: 34.6666666667%; }
  .offset-by-five.column,
  .offset-by-five.columns         { margin-left: 43.3333333333%; }
  .offset-by-six.column,
  .offset-by-six.columns          { margin-left: 52%;            }
  .offset-by-seven.column,
  .offset-by-seven.columns        { margin-left: 60.6666666667%; }
  .offset-by-eight.column,
  .offset-by-eight.columns        { margin-left: 69.3333333333%; }
  .offset-by-nine.column,
  .offset-by-nine.columns         { margin-left: 78.0%;          }
  .offset-by-ten.column,
  .offset-by-ten.columns          { margin-left: 86.6666666667%; }
  .offset-by-eleven.column,
  .offset-by-eleven.columns       { margin-left: 95.3333333333%; }

  .offset-by-one-third.column,
  .offset-by-one-third.columns    { margin-left: 34.6666666667%; }
  .offset-by-two-thirds.column,
  .offset-by-two-thirds.columns   { margin-left: 69.3333333333%; }

  .offset-by-one-half.column,
  .offset-by-one-half.columns     { margin-left: 52%; }

}
html {
  font-size: 62.5%; }
body {
  font-size: 1.5em; /* currently ems cause chrome bug misinterpreting rems on body element */
  line-height: 1.6;
  font-weight: 400;
  font-family: "Raleway", "HelveticaNeue", "Helvetica Neue", Helvetica, Arial, sans-serif;
  color: #222; }
h1, h2, h3, h4, h5, h6 {
  margin-top: 0;
  margin-bottom: 2rem;
  font-weight: 300; }
h1 { font-size: 3.6rem; line-height: 1.2;  letter-spacing: -.1rem;}
h2 { font-size: 3.4rem; line-height: 1.25; letter-spacing: -.1rem; }
h3 { font-size: 3.2rem; line-height: 1.3;  letter-spacing: -.1rem; }
h4 { font-size: 2.8rem; line-height: 1.35; letter-spacing: -.08rem; }
h5 { font-size: 2.4rem; line-height: 1.5;  letter-spacing: -.05rem; }
h6 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }

p {
  margin-top: 0; }
a {
  color: #1EAEDB; }
a:hover {
  color: #0FA0CE; }
.button,
button,
input[type="submit"],
input[type="reset"],
input[type="button"] {
  display: inline-block;
  height: 38px;
  padding: 0 30px;
  color: #555;
  text-align: center;
  font-size: 11px;
  font-weight: 600;
  line-height: 38px;
  letter-spacing: .1rem;
  text-transform: uppercase;
  text-decoration: none;
  white-space: nowrap;
  background-color: transparent;
  border-radius: 4px;
  border: 1px solid #bbb;
  cursor: pointer;
  box-sizing: border-box; }
.button:hover,
button:hover,
input[type="submit"]:hover,
input[type="reset"]:hover,
input[type="button"]:hover,
.button:focus,
button:focus,
input[type="submit"]:focus,
input[type="reset"]:focus,
input[type="button"]:focus {
  color: #333;
  border-color: #888;
  outline: 0; }
.button.button-primary,
button.button-primary,
input[type="submit"].button-primary,
input[type="reset"].button-primary,
input[type="button"].button-primary {
  color: #FFF;
  background-color: #33C3F0;
  border-color: #33C3F0; }
.button.button-primary:hover,
button.button-primary:hover,
input[type="submit"].button-primary:hover,
input[type="reset"].button-primary:hover,
input[type="button"].button-primary:hover,
.button.button-primary:focus,
button.button-primary:focus,
input[type="submit"].button-primary:focus,
input[type="reset"].button-primary:focus,
input[type="button"].button-primary:focus {
  color: #FFF;
  background-color: #1EAEDB;
  border-color: #1EAEDB; }
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea,
select {
  height: 38px;
  padding: 6px 10px; /* The 6px vertically centers text on FF, ignored by Webkit */
  background-color: #fff;
  border: 1px solid #D1D1D1;
  border-radius: 4px;
  box-shadow: none;
  box-sizing: border-box; }
/* Removes awkward default styles on some inputs for iOS */
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea {
  -webkit-appearance: none;
     -moz-appearance: none;
          appearance: none; }
textarea {
  min-height: 65px;
  padding-top: 6px;
  padding-bottom: 6px; }
input[type="email"]:focus,
input[type="number"]:focus,
input[type="search"]:focus,
input[type="text"]:focus,
input[type="tel"]:focus,
input[type="url"]:focus,
input[type="password"]:focus,
textarea:focus,
select:focus {
  border: 1px solid #33C3F0;
  outline: 0; }
label,
legend {
  display: block;
  margin-bottom: .5rem;
  font-weight: 600; }
fieldset {
  padding: 0;
  border-width: 0; }
input[type="checkbox"],
input[type="radio"] {
  display: inline; }
label > .label-body {
  display: inline-block;
  margin-left: .5rem;
  font-weight: normal; }
ul {
  list-style: circle; }
ol {
  list-style: decimal; }
ul ul,
ul ol,
ol ol,
ol ul {
  margin: 1.5rem 0 1.5rem 3rem;
  font-size: 90%; }
li > p {margin : 0;}
th,
td {
  padding: 12px 15px;
  text-align: left;
  border-bottom: 1px solid #E1E1E1; }
th:first-child,
td:first-child {
  padding-left: 0; }
th:last-child,
td:last-child {
  padding-right: 0; }
button,
.button {
  margin-bottom: 1rem; }
input,
textarea,
select,
fieldset {
  margin-bottom: 1.5rem; }
pre,
blockquote,
dl,
figure,
table,
p,
ul,
ol,
form {
  margin-bottom: 1.0rem; }
.u-full-width {
  width: 100%;
  box-sizing: border-box; }
.u-max-full-width {
  max-width: 100%;
  box-sizing: border-box; }
.u-pull-right {
  float: right; }
.u-pull-left {
  float: left; }
hr {
  margin-top: 3rem;
  margin-bottom: 3.5rem;
  border-width: 0;
  border-top: 1px solid #E1E1E1; }
.container:after,
.row:after,
.u-cf {
  content: "";
  display: table;
  clear: both; }

pre {
  display: block;
  padding: 9.5px;
  margin: 0 0 10px;
  font-size: 13px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre.hljl {
  margin: 0 0 10px;
  display: block;
  background: #f5f5f5;
  border-radius: 4px;
  padding : 5px;
}

pre.output {
  background: #ffffff;
}

pre.code {
  background: #ffffff;
}

pre.julia-error {
  color : red
}

code,
kbd,
pre,
samp {
  font-family: Menlo, Monaco, Consolas, "Courier New", monospace;
  font-size: 13px;
}


@media (min-width: 400px) {}
@media (min-width: 550px) {}
@media (min-width: 750px) {}
@media (min-width: 1000px) {}
@media (min-width: 1200px) {}

h1.title {margin-top : 20px}
img {max-width : 100%}
div.title {text-align: center;}

  </style>
</HEAD>

<BODY>
  <div class ="container">
    <div class = "row">
      <div class = "col-md-12 twelve columns">
        <div class="title">
          <h1 class="title">Variational Inference</h1>
          
          
        </div>

        <h1>Overview</h1>
<p>In this post we&#39;ll have a look at what&#39;s known as <strong>variational inference &#40;VI&#41;</strong>, a family of <em>approximate</em> Bayesian inference methods. In particular, we will focus on one of the more standard VI methods called <strong>Automatic Differentiation Variational Inference &#40;ADVI&#41;</strong>.</p>
<p>Here we&#39;ll have a look at the theory behind VI, but if you&#39;re interested in how to use ADVI in Turing.jl, <a href="../../tutorials/09-variational-inference">check out this tutorial</a>.</p>
<h1>Motivation</h1>
<p>In Bayesian inference one usually specifies a model as follows: given data <span class="math">$\\{x_i\\}_{i = 1}^n$</span>,</p>
<p class="math">\[
\begin{align*}
  \text{prior:} \quad z &\sim p(z)   \\\\
  \text{likelihood:} \quad x_i &\overset{\text{i.i.d.}}{\sim} p(x \mid z) \quad  \text{where} \quad i = 1, \dots, n
\end{align*}
\]</p>
<p>where <span class="math">$\overset{\text{i.i.d.}}{\sim}$</span> denotes that the samples are identically independently distributed. Our goal in Bayesian inference is then to find the <em>posterior</em></p>
<p class="math">\[
p(z \mid \\{ x\_i \\}\_{i = 1}^n) \propto p(z) \prod\_{i=1}^{n} p(x\_i \mid z).
\]</p>
<p>In general one cannot obtain a closed form expression for <span class="math">$p(z \mid \\{ x\_i \\}\_{i = 1}^n)$</span>, but one might still be able to <em>sample</em> from <span class="math">$p(z \mid \\{ x\_i \\}\_{i = 1}^n)$</span> with guarantees of converging to the target posterior <span class="math">$p(z \mid \\{ x\_i \\}\_{i = 1}^n)$</span> as the number of samples go to <span class="math">$\infty$</span>, e.g. MCMC.</p>
<p>As you are hopefully already aware, Turing.jl provides a lot of different methods with asymptotic exactness guarantees that we can apply to such a problem&#33;</p>
<p>Unfortunately, these unbiased samplers can be prohibitively expensive to run. As the model <span class="math">$p$</span> increases in complexity, the convergence of these unbiased samplers can slow down dramatically. Still, in the <em>infinite</em> limit, these methods should converge to the true posterior&#33; But infinity is fairly large, like, <em>at least</em> more than 12, so this might take a while.</p>
<p>In such a case it might be desirable to sacrifice some of these asymptotic guarantees, and instead <em>approximate</em> the posterior <span class="math">$p(z \mid \\{ x_i \\}_{i = 1}^n)$</span> using some other model which we&#39;ll denote <span class="math">$q(z)$</span>.</p>
<p>There are multiple approaches to take in this case, one of which is <strong>variational inference &#40;VI&#41;</strong>.</p>
<h1>Variational Inference &#40;VI&#41;</h1>
<p>In VI, we&#39;re looking to approximate <span class="math">$p(z \mid \\{ x_i \\}_{i = 1}^n )$</span> using some <em>approximate</em> or <em>variational</em> posterior <span class="math">$q(z)$</span>.</p>
<p>To approximate something you need a notion of what &quot;close&quot; means. In the context of probability densities a standard such &quot;measure&quot; of closeness is the <em>Kullback-Leibler &#40;KL&#41; divergence</em> , though this is far from the only one. The KL-divergence is defined between two densities <span class="math">$q(z)$</span> and <span class="math">$p(z \mid \\{ x_i \\}_{i = 1}^n)$</span> as</p>
<p class="math">\[
\begin{align*}
  \mathrm{D\_{KL}} \left( q(z), p(z \mid \\{ x\_i \\}\_{i = 1}^n) \right) &= \int \log \left( \frac{q(z)}{\prod\_{i = 1}^n p(z \mid x\_i)} \right) q(z) \mathrm{d}{z} \\\\
  &= \mathbb{E}\_{z \sim q(z)} \left[ \log q(z) - \sum\_{i = 1}^n \log p(z \mid x\_i) \right] \\\\
  &= \mathbb{E}\_{z \sim q(z)} \left[ \log q(z) \right] - \sum\_{i = 1}^n \mathbb{E}\_{z \sim q(z)} \left[ \log p(z \mid x\_i) \right].
\end{align*}
\]</p>
<p>It&#39;s worth noting that unfortunately the KL-divergence is <em>not</em> a metric/distance in the analysis-sense due to its lack of symmetry. On the other hand, it turns out that minimizing the KL-divergence that it&#39;s actually equivalent to maximizing the log-likelihood&#33; Also, under reasonable restrictions on the densities at hand,</p>
<p class="math">\[
\mathrm{D\_{KL}}\left(q(z), p(z \mid \\{ x\_i \\}\_{i = 1}^n) \right) = 0 \quad \iff \quad q(z) = p(z \mid \\{ x\_i \\}\_{i = 1}^n), \quad \forall z.
\]</p>
<p>Therefore one could &#40;and we will&#41; attempt to approximate <span class="math">$p(z \mid \\{ x_i \\}_{i = 1}^n)$</span> using a density <span class="math">$q(z)$</span> by minimizing the KL-divergence between these two&#33;</p>
<p>One can also show that <span class="math">$\mathrm{D_{KL}} \ge 0$</span>, which we&#39;ll need later. Finally notice that the KL-divergence is only well-defined when in fact <span class="math">$q(z)$</span> is zero everywhere <span class="math">$p(z \mid \\{ x_i \\}_{i = 1}^n)$</span> is zero, i.e.</p>
<p class="math">\[
\mathrm{supp}\left(q(z)\right) \subseteq \mathrm{supp}\left(p(z \mid x)\right).
\]</p>
<p>Otherwise, there might be a point <span class="math">$z_0 \sim q(z)$</span> such that <span class="math">$p(z_0 \mid \\{ x_i \\}_{i = 1}^n) = 0$</span>, resulting in <span class="math">$\log\left(\frac{q(z)}{0}\right)$</span> which doesn&#39;t make sense&#33;</p>
<p>One major problem: as we can see in the definition of the KL-divergence, we need <span class="math">$p(z \mid \\{ x_i \\}_{i = 1}^n)$</span> for any <span class="math">$z$</span> if we want to compute the KL-divergence between this and <span class="math">$q(z)$</span>. We don&#39;t have that. The entire reason we even do Bayesian inference is that we don&#39;t know the posterior&#33; Cleary this isn&#39;t going to work. <em>Or is it?&#33;</em></p>
<h2>Computing KL-divergence without knowing the posterior</h2>
<p>First off, recall that</p>
<p class="math">\[
p(z \mid x\_i) = \frac{p(x\_i, z)}{p(x\_i)}
\]</p>
<p>so we can write</p>
<p class="math">\[
\begin{align*}
\mathrm{D\_{KL}} \left( q(z), p(z \mid \\{ x\_i \\}\_{i = 1}^n) \right) &= \mathbb{E}\_{z \sim q(z)} \left[ \log q(z) \right] - \sum\_{i = 1}^n \mathbb{E}\_{z \sim q(z)} \left[ \log p(x\_i, z) - \log p(x\_i) \right] \\\\
    &= \mathbb{E}\_{z \sim q(z)} \left[ \log q(z) \right] - \sum\_{i = 1}^n \mathbb{E}\_{z \sim q(z)} \left[ \log p(x\_i, z) \right] + \sum\_{i = 1}^n \mathbb{E}\_{z \sim q(z)} \left[ \log p(x_i) \right] \\\\
    &= \mathbb{E}\_{z \sim q(z)} \left[ \log q(z) \right] - \sum\_{i = 1}^n \mathbb{E}\_{z \sim q(z)} \left[ \log p(x\_i, z) \right] + \sum\_{i = 1}^n \log p(x\_i),
\end{align*}
\]</p>
<p>where in the last equality we used the fact that <span class="math">$p(x_i)$</span> is independent of <span class="math">$z$</span>.</p>
<p>Now you&#39;re probably thinking &quot;Oh great&#33; Now you&#39;ve introduced <span class="math">$p(x_i)$</span> which we <em>also</em> can&#39;t compute &#40;in general&#41;&#33;&quot;. Woah. Calm down human. Let&#39;s do some more algebra. The above expression can be rearranged to</p>
<p class="math">\[
\mathrm{D\_{KL}} \left( q(z), p(z \mid \\{ x\_i \\}\_{i = 1}^n) \right) + \underbrace{\sum\_{i = 1}^n \mathbb{E}\_{z \sim q(z)} \left[ \log p(x\_i, z) \right] - \mathbb{E}\_{z \sim q(z)} \left[ \log q(z) \right]}\_{=: \mathrm{ELBO}(q)} = \underbrace{\sum\_{i = 1}^n \mathbb{E}\_{z \sim q(z)} \left[ \log p(x\_i) \right]}\_{\text{constant}}.
\]</p>
<p>See? The left-hand side is <em>constant</em> and, as we mentioned before, <span class="math">$\mathrm{D_{KL}} \ge 0$</span>. What happens if we try to <em>maximize</em> the term we just gave the completely arbitrary name <span class="math">$\mathrm{ELBO}$</span>? Well, if <span class="math">$\mathrm{ELBO}$</span> goes up while <span class="math">$p(x_i)$</span> stays constant then <span class="math">$\mathrm{D_{KL}}$</span> <em>has to</em> go down&#33; That is, the <span class="math">$q(z)$</span> which <em>minimizes</em> the KL-divergence is the same <span class="math">$q(z)$</span> which <em>maximizes</em> <span class="math">$\mathrm{ELBO}(q)$</span>:</p>
<p class="math">\[
\underset{q}{\mathrm{argmin}} \  \mathrm{D\_{KL}} \left( q(z), p(z \mid \\{ x\_i \\}\_{i = 1}^n) \right) = \underset{q}{\mathrm{argmax}} \ \mathrm{ELBO}(q)
\]</p>
<p>where</p>
<p class="math">\[
\begin{align*}
\mathrm{ELBO}(q) &:= \left( \sum\_{i = 1}^n \mathbb{E}\_{z \sim q(z)} \left[ \log p(x\_i, z) \right]  \right) - \mathbb{E}\_{z \sim q(z)} \left[ \log q(z) \right] \\\\
    &= \left( \sum\_{i = 1}^n \mathbb{E}\_{z \sim q(z)} \left[ \log p(x\_i, z) \right] \right) + \mathbb{H}\left( q(z) \right)
\end{align*}
\]</p>
<p>and <span class="math">$\mathbb{H} \left(q(z) \right)$</span> denotes the <a href="https://www.wikiwand.com/en/Differential_entropy">&#40;differential&#41; entropy</a> of <span class="math">$q(z)$</span>.</p>
<p>Assuming joint <span class="math">$p(x_i, z)$</span> and the entropy <span class="math">$\mathbb{H}\left(q(z)\right)$</span> are both tractable, we can use a Monte-Carlo for the remaining expectation. This leaves us with the following tractable expression</p>
<p class="math">\[
\underset{q}{\mathrm{argmin}} \ \mathrm{D\_{KL}} \left( q(z), p(z \mid \\{ x\_i \\}\_{i = 1}^n) \right) \approx \underset{q}{\mathrm{argmax}} \ \widehat{\mathrm{ELBO}}(q)
\]</p>
<p>where</p>
<p class="math">\[
\widehat{\mathrm{ELBO}}(q) = \frac{1}{m} \left( \sum\_{k = 1}^m \sum\_{i = 1}^n \log p(x\_i, z\_k) \right) + \mathbb{H} \left(q(z)\right) \quad \text{where} \quad z\_k \sim q(z) \quad \forall k = 1, \dots, m.
\]</p>
<p>Hence, as long as we can sample from <span class="math">$q(z)$</span> somewhat efficiently, we can indeed minimize the KL-divergence&#33; Neat, eh?</p>
<p>Sidenote: in the case where <span class="math">$q(z)$</span> is tractable but <span class="math">$\mathbb{H} \left(q(z) \right)$</span> is <em>not</em> , we can use an Monte-Carlo estimate for this term too but this generally results in a higher-variance estimate.</p>
<p>Also, I fooled you real good: the ELBO <em>isn&#39;t</em> an arbitrary name, hah&#33; In fact it&#39;s an abbreviation for the <strong>expected lower bound &#40;ELBO&#41;</strong> because it, uhmm, well, it&#39;s the <em>expected</em> lower bound &#40;remember <span class="math">$\mathrm{D_{KL}} \ge 0$</span>&#41;. Yup.</p>
<h2>Maximizing the ELBO</h2>
<p>Finding the optimal <span class="math">$q$</span> over <em>all</em> possible densities of course isn&#39;t feasible. Instead we consider a family of <em>parameterized</em> densities <span class="math">$\mathscr{D}\_{\Theta}$</span> where <span class="math">$\Theta$</span> denotes the space of possible parameters. Each density in this family <span class="math">$q\_{\theta} \in \mathscr{D}\_{\Theta}$</span> is parameterized by a unique <span class="math">$\theta \in \Theta$</span>. Moreover, we&#39;ll assume</p>
<ol>
<li><p><span class="math">$q\_{\theta}(z)$</span>, i.e. evaluating the probability density <span class="math">$q$</span> at any point <span class="math">$z$</span>, is differentiable</p>
</li>
<li><p><span class="math">$z \sim q\_{\theta}(z)$</span>, i.e. the process of sampling from <span class="math">$q\_{\theta}(z)$</span>, is differentiable</p>
</li>
</ol>
<p>&#40;1&#41; is fairly straight-forward, but &#40;2&#41; is a bit tricky. What does it even mean for a <em>sampling process</em> to be differentiable? This is quite an interesting problem in its own right and would require something like a <a href="https://arxiv.org/abs/1906.10652">50-page paper to properly review the different approaches &#40;highly recommended read&#41;</a>.</p>
<p>We&#39;re going to make use of a particular such approach which goes under a bunch of different names: <em>reparametrization trick</em>, <em>path derivative</em>, etc. This refers to making the assumption that all elements <span class="math">$q\_{\theta} \in \mathscr{Q}\_{\Theta}$</span> can be considered as reparameterizations of some base density, say <span class="math">$\bar{q}(z)$</span>. That is, if <span class="math">$q\_{\theta} \in \mathscr{Q}\_{\Theta}$</span> then</p>
<p class="math">\[
z \sim q\_{\theta}(z) \quad \iff \quad z := g\_{\theta}(\tilde{z}) \quad \text{where} \quad \bar{z} \sim \bar{q}(z)
\]</p>
<p>for some function <span class="math">$g\_{\theta}$</span> differentiable wrt. <span class="math">$\theta$</span>. So all <span class="math">$q_{\theta} \in \mathscr{Q}\_{\Theta}$</span> are using the <em>same</em> reparameterization-function <span class="math">$g$</span> but each <span class="math">$q\_{\theta}$</span> correspond to different choices of <span class="math">$\theta$</span> for <span class="math">$f\_{\theta}$</span>.</p>
<p>Under this assumption we can differentiate the sampling process by taking the derivative of <span class="math">$g\_{\theta}$</span> wrt. <span class="math">$\theta$</span>, and thus we can differentiate the entire <span class="math">$\widehat{\mathrm{ELBO}}(q\_{\theta})$</span> wrt. <span class="math">$\theta$</span>&#33; With the gradient available we can either try to solve for optimality either by setting the gradient equal to zero or maximize <span class="math">$\widehat{\mathrm{ELBO}}(q\_{\theta})$</span> stepwise by traversing <span class="math">$\mathscr{Q}\_{\Theta}$</span> in the direction of steepest ascent. For the sake of generality, we&#39;re going to go with the stepwise approach.</p>
<p>With all this nailed down, we eventually reach the section on <strong>Automatic Differentiation Variational Inference &#40;ADVI&#41;</strong>.</p>
<h2>Automatic Differentiation Variational Inference &#40;ADVI&#41;</h2>
<p>So let&#39;s revisit the assumptions we&#39;ve made at this point:</p>
<ol>
<li><p>The variational posterior <span class="math">$q\_{\theta}$</span> is in a parameterized family of densities denoted <span class="math">$\mathscr{Q}\_{\Theta}$</span>, with <span class="math">$\theta \in \Theta$</span>.</p>
</li>
<li><p><span class="math">$\mathscr{Q}\_{\Theta}$</span> is a space of <em>reparameterizable</em> densities with <span class="math">$\bar{q}(z)$</span> as the base-density.</p>
</li>
<li><p>The parameterization function <span class="math">$g\_{\theta}$</span> is differentiable wrt. <span class="math">$\theta$</span>.</p>
</li>
<li><p>Evaluation of the probability density <span class="math">$q\_{\theta}(z)$</span> is differentiable wrt. <span class="math">$\theta$</span>.</p>
</li>
<li><p><span class="math">$\mathbb{H}\left(q\_{\theta}(z)\right)$</span> is tractable.</p>
</li>
<li><p>Evaluation of the joint density <span class="math">$p(x, z)$</span> is tractable and differentiable wrt. <span class="math">$z$</span></p>
</li>
<li><p>The support of <span class="math">$q(z)$</span> is a subspace of the support of <span class="math">$p(z \mid x)$</span> : <span class="math">$\mathrm{supp}\left(q(z)\right) \subseteq \mathrm{supp}\left(p(z \mid x)\right)$</span>.</p>
</li>
</ol>
<p>All of these are not <em>necessary</em> to do VI, but they are very convenient and results in a fairly flexible approach. One distribution which has a density satisfying all of the above assumptions <em>except</em> &#40;7&#41; &#40;we&#39;ll get back to this in second&#41; for any tractable and differentiable <span class="math">$p(z \mid \\{ x\_i \\}\_{i = 1}^n)$</span> is the good ole&#39; Gaussian/normal distribution:</p>
<p class="math">\[
z \sim \mathcal{N}(\mu, \Sigma) \quad \iff \quad z = g\_{\mu, L}(\bar{z}) := \mu + L^T \tilde{z} \quad \text{where} \quad \bar{z} \sim \bar{q}(z) := \mathcal{N}(1\_d, I\_{d \times d})
\]</p>
<p>where <span class="math">$\Sigma = L L^T,$</span> with <span class="math">$L$</span> obtained from the Cholesky-decomposition. Abusing notation a bit, we&#39;re going to write</p>
<p class="math">\[
\theta = (\mu, \Sigma) := (\mu\_1, \dots, \mu\_d, L\_{11}, \dots, L\_{1, d}, L\_{2, 1}, \dots, L\_{2, d}, \dots, L\_{d, 1}, \dots, L\_{d, d}).
\]</p>
<p>With this assumption we finally have a tractable expression for <span class="math">$\widehat{\mathrm{ELBO}}(q_{\mu, \Sigma})$</span>&#33; Well, assuming &#40;7&#41; is holds. Since a Gaussian has non-zero probability on the entirety of <span class="math">$\mathbb{R}^d$</span>, we also require <span class="math">$p(z \mid \\{ x_i \\}_{i = 1}^n)$</span> to have non-zero probability on all of <span class="math">$\mathbb{R}^d$</span>.</p>
<p>Though not necessary, we&#39;ll often make a <em>mean-field</em> assumption for the variational posterior <span class="math">$q(z)$</span>, i.e. assume independence between the latent variables. In this case, we&#39;ll write</p>
<p class="math">\[
\theta = (\mu, \sigma^2) := (\mu\_1, \dots, \mu\_d, \sigma\_1^2, \dots, \sigma\_d^2).
\]</p>
<h3>Examples</h3>
<p>As a &#40;trivial&#41; example we could apply the approach described above to is the following generative model for <span class="math">$p(z \mid \\{ x_i \\}\_{i = 1}^n)$</span>:</p>
<p class="math">\[
\begin{align*}
    m &\sim \mathcal{N}(0, 1) \\\\
    x\_i &\overset{\text{i.i.d.}}{=} \mathcal{N}(m, 1), \quad i = 1, \dots, n.
\end{align*}
\]</p>
<p>In this case <span class="math">$z = m$</span> and we have the posterior defined <span class="math">$p(m \mid \\{ x\_i \\}\_{i = 1}^n) = p(m) \prod\_{i = 1}^n p(x\_i \mid m)$</span>. Then the variational posterior would be</p>
<p class="math">\[
q\_{\mu, \sigma} = \mathcal{N}(\mu, \sigma^2), \quad \text{where} \quad \mu \in \mathbb{R}, \ \sigma^2 \in \mathbb{R}^{ + }.
\]</p>
<p>And since prior of <span class="math">$m$</span>, <span class="math">$\mathcal{N}(0, 1)$</span>, has non-zero probability on the entirety of <span class="math">$\mathbb{R}$</span>, same as <span class="math">$q(m)$</span>, i.e. assumption &#40;7&#41; above holds, everything is fine and life is good.</p>
<p>But what about this generative model for <span class="math">$p(z \mid \\{ x_i \\}_{i = 1}^n)$</span>:</p>
<p class="math">\[
\begin{align*}
    s &\sim \mathrm{InverseGamma}(2, 3), \\\\
    m &\sim \mathcal{N}(0, s), \\\\
    x\_i &\overset{\text{i.i.d.}}{=} \mathcal{N}(m, s), \quad i = 1, \dots, n,
\end{align*}
\]</p>
<p>with posterior <span class="math">$p(s, m \mid \\{ x\_i \\}\_{i = 1}^n) = p(s) p(m \mid s) \prod\_{i = 1}^n p(x\_i \mid s, m)$</span> and the mean-field variational posterior <span class="math">$q(s, m)$</span> will be</p>
<p class="math">\[
q\_{\mu\_1, \mu\_2, \sigma\_1^2, \sigma\_2^2}(s, m) = p\_{\mathcal{N}(\mu\_1, \sigma\_1^2)}(s)\ p\_{\mathcal{N}(\mu\_2, \sigma\_2^2)}(m),
\]</p>
<p>where we&#39;ve denoted the evaluation of the probability density of a Gaussian as <span class="math">$p_{\mathcal{N}(\mu, \sigma^2)}(x)$</span>.</p>
<p>Observe that <span class="math">$\mathrm{InverseGamma}(2, 3)$</span> has non-zero probability only on <span class="math">$\mathbb{R}^{ + } := (0, \infty)$</span> which is clearly not all of <span class="math">$\mathbb{R}$</span> like <span class="math">$q(s, m)$</span> has, i.e.</p>
<p class="math">\[
\mathrm{supp} \left( q(s, m) \right) \not\subseteq \mathrm{supp} \left( p(z \mid \\{ x\_i \\}\_{i = 1}^n) \right).
\]</p>
<p>Recall from the definition of the KL-divergence that when this is the case, the KL-divergence isn&#39;t well defined. This gets us to the <em>automatic</em> part of ADVI.</p>
<h3>&quot;Automatic&quot;? How?</h3>
<p>For a lot of the standard &#40;continuous&#41; densities <span class="math">$p$</span> we can actually construct a probability density <span class="math">$\tilde{p}$</span> with non-zero probability on all of <span class="math">$\mathbb{R}$</span> by <em>transforming</em> the &quot;constrained&quot; probability density <span class="math">$p$</span> to <span class="math">$\tilde{p}$</span>. In fact, in these cases this is a one-to-one relationship. As we&#39;ll see, this helps solve the support-issue we&#39;ve been going on and on about.</p>
<h4>Transforming densities using change of variables</h4>
<p>If we want to compute the probability of <span class="math">$x$</span> taking a value in some set <span class="math">$A \subseteq \mathrm{supp} \left( p(x) \right)$</span>, we have to integrate <span class="math">$p(x)$</span> over <span class="math">$A$</span>, i.e.</p>
<p class="math">\[
\mathbb{P}_p(x \in A) = \int_A p(x) \mathrm{d}x.
\]</p>
<p>This means that if we have a differentiable bijection <span class="math">$f: \mathrm{supp} \left( q(x) \right) \to \mathbb{R}^d$</span> with differentiable inverse <span class="math">$f^{-1}: \mathbb{R}^d \to \mathrm{supp} \left( p(x) \right)$</span>, we can perform a change of variables</p>
<p class="math">\[
\mathbb{P}\_p(x \in A) = \int\_{f^{-1}(A)} p \left(f^{-1}(y) \right) \ \left| \det \mathcal{J}\_{f^{-1}}(y) \right| \mathrm{d}y,
\]</p>
<p>where <span class="math">$\mathcal{J}_{f^{-1}}(x)$</span> denotes the jacobian of <span class="math">$f^{-1}$</span> evaluated at <span class="math">$x$</span>. Observe that this defines a probability distribution</p>
<p class="math">\[
\mathbb{P}\_{\tilde{p}}\left(y \in f^{-1}(A) \right) = \int\_{f^{-1}(A)} \tilde{p}(y) \mathrm{d}y,
\]</p>
<p>since <span class="math">$f^{-1}\left(\mathrm{supp} (p(x)) \right) = \mathbb{R}^d$</span> which has probability 1. This probability distribution has <em>density</em> <span class="math">$\tilde{p}(y)$</span> with <span class="math">$\mathrm{supp} \left( \tilde{p}(y) \right) = \mathbb{R}^d$</span>, defined</p>
<p class="math">\[
\tilde{p}(y) = p \left( f^{-1}(y) \right) \ \left| \det \mathcal{J}\_{f^{-1}}(y) \right|
\]</p>
<p>or equivalently</p>
<p class="math">\[
\tilde{p} \left( f(x) \right) = \frac{p(x)}{\big| \det \mathcal{J}\_{f}(x) \big|}
\]</p>
<p>due to the fact that</p>
<p class="math">\[
\big| \det \mathcal{J}\_{f^{-1}}(y) \big| = \big| \det \mathcal{J}\_{f}(x) \big|^{-1}
\]</p>
<p><em>Note: it&#39;s also necessary that the log-abs-det-jacobian term is non-vanishing. This can for example be accomplished by assuming <span class="math">$f$</span> to also be elementwise monotonic.</em></p>
<h4>Back to VI</h4>
<p>So why is this is useful? Well, we&#39;re looking to generalize our approach using a normal distribution to cases where the supports don&#39;t match up. How about defining <span class="math">$q(z)$</span> by</p>
<p class="math">\[
\begin{align*}
  \eta &\sim \mathcal{N}(\mu, \Sigma), \\\\
  z &= f^{-1}(\eta),
\end{align*}
\]</p>
<p>where <span class="math">$f^{-1}: \mathbb{R}^d \to \mathrm{supp} \left( p(z \mid x) \right)$</span> is a differentiable bijection with differentiable inverse. Then <span class="math">$z \sim q_{\mu, \Sigma}(z) \implies z \in \mathrm{supp} \left( p(z \mid x) \right)$</span> as we wanted. The resulting variational density is</p>
<p class="math">\[
q\_{\mu, \Sigma}(z) = p\_{\mathcal{N}(\mu, \Sigma)}\left( f(z) \right) \ \big| \det \mathcal{J}\_{f}(z) \big|.
\]</p>
<p>Note that the way we&#39;ve constructed <span class="math">$q(z)$</span> here is basically a reverse of the approach we described above. Here we sample from a distribution with support on <span class="math">$\mathbb{R}$</span> and transform <em>to</em> <span class="math">$\mathrm{supp} \left( p(z \mid x) \right)$</span>.</p>
<p>If we want to write the ELBO explicitly in terms of <span class="math">$\eta$</span> rather than <span class="math">$z$</span>, the first term in the ELBO becomes</p>
<p class="math">\[
\begin{align*}
  \mathbb{E}\_{z \sim q_{\mu, \Sigma}(z)} \left[ \log p(x\_i, z) \right] &= \mathbb{E}\_{\eta \sim \mathcal{N}(\mu, \Sigma)} \Bigg[ \log \frac{p\left(x\_i, f^{-1}(\eta) \right)}{\big| \det \mathcal{J}_{f^{-1}}(\eta) \big|} \Bigg] \\\\
  &= \mathbb{E}\_{\eta \sim \mathcal{N}(\mu, \Sigma)} \left[ \log p\left(x\_i, f^{-1}(\eta) \right) \right] - \mathbb{E}\_{\eta \sim \mathcal{N}(\mu, \Sigma)} \left[ \left| \det \mathcal{J}\_{f^{-1}}(\eta) \right| \right].
\end{align*}
\]</p>
<p>The entropy is invariant under change of variables, thus <span class="math">$\mathbb{H} \left(q\_{\mu, \Sigma}(z)\right)$</span> is simply the entropy of the normal distribution which is known analytically.</p>
<p>Hence, the resulting empirical estimate of the ELBO is</p>
<p class="math">\[
\begin{align*}
\widehat{\mathrm{ELBO}}(q\_{\mu, \Sigma}) &= \frac{1}{m} \left( \sum\_{k = 1}^m \sum\_{i = 1}^n \left(\log p\left(x\_i, f^{-1}(\eta_k)\right) - \log \big| \det \mathcal{J}\_{f^{-1}}(\eta\_k) \big| \right) \right) + \mathbb{H} \left(p\_{\mathcal{N}(\mu, \Sigma)}(z)\right) \\\\
& \text{where} \quad z\_k  \sim \mathcal{N}(\mu, \Sigma) \quad \forall k = 1, \dots, m
\end{align*}.
\]</p>
<p>And maximizing this wrt. <span class="math">$\mu$</span> and <span class="math">$\Sigma$</span> is what&#39;s referred to as <strong>Automatic Differentiation Variational Inference &#40;ADVI&#41;</strong>&#33;</p>
<p>Now if you want to try it out, <a href="../../tutorials/09-variational-inference">check out the tutorial on how to use ADVI in Turing.jl</a>&#33;</p>


        <HR/>
        <div class="footer">
          <p>
            Published from <a href="variational_inference.jmd">variational_inference.jmd</a>
            using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.12 on 2023-08-12.
          </p>
        </div>
      </div>
    </div>
  </div>
</BODY>

</HTML>
